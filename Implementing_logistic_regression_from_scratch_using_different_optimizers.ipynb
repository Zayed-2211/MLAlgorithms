{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries"
      ],
      "metadata": {
        "id": "Q39F7QqT8a_j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hj1pN3o-08DS"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Load MNIST dataset"
      ],
      "metadata": {
        "id": "zmpKp0BN86GA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MNIST dataset\n",
        "mnist = fetch_openml('mnist_784')"
      ],
      "metadata": {
        "id": "gSLwxNlEb3B9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25fc431c-fd1a-4464-fd85-b8124539bd1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 2. Subset your data to use only class 0 and class 1 for the next steps."
      ],
      "metadata": {
        "id": "4j-5fPPu9tgp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81DWR5Rj7nHm"
      },
      "outputs": [],
      "source": [
        "# Convert data and labels to numpy arrays\n",
        "X = np.array(mnist.data, dtype='float32')\n",
        "y = np.array(mnist.target, dtype='int64')\n",
        "\n",
        "# Subset the data to include only class 0 and class 1\n",
        "X = X[(y == 0) | (y == 1)]\n",
        "y = y[(y == 0) | (y == 1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#3. Standardize your dataset"
      ],
      "metadata": {
        "id": "iU5aaqAy9wzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize_dataset(data):\n",
        "    # Calculate the mean and standard deviation of each feature\n",
        "    means = np.mean(data)\n",
        "    stds = np.std(data)\n",
        "\n",
        "    # Subtract the mean from each feature and divide by its standard deviation\n",
        "    standardized_data = (data - means) / stds\n",
        "\n",
        "    return standardized_data"
      ],
      "metadata": {
        "id": "e3LUY3pIoVEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize the dataset\n",
        "X = standardize_dataset(X)"
      ],
      "metadata": {
        "id": "3p7DdVUYpglo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Divide data into training and validation set"
      ],
      "metadata": {
        "id": "l0JtlNJA9-7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_val_test_split(X, y, train_size=0.6, val_size=0.2, test_size=0.2):\n",
        "\n",
        "    # Convert proportions to sizes\n",
        "    train_size = int(train_size * X.shape[0])\n",
        "    val_size = int(val_size * X.shape[0])\n",
        "    test_size = int(test_size * X.shape[0])\n",
        "\n",
        "    # Split the data and labels into training, validation, and test sets\n",
        "    X_train = X[:train_size]\n",
        "    y_train = y[:train_size]\n",
        "    X_val = X[train_size:train_size+val_size]\n",
        "    y_val = y[train_size:train_size+val_size]\n",
        "    X_test = X[train_size+val_size:]\n",
        "    y_test = y[train_size+val_size:]\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test"
      ],
      "metadata": {
        "id": "wVkeou8f-F4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train, X_val, y_val, X_test, y_test = train_val_test_split(X, y, train_size=0.6, val_size=0.2, test_size=0.2)"
      ],
      "metadata": {
        "id": "Zur2avXi-eUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(y_pred, y_test):\n",
        "    return (np.sum(y_pred==y_test)/len(y_test))*100"
      ],
      "metadata": {
        "id": "fRmbFboawjQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f1_score(y_pred, y_test):\n",
        "    # Calculate true positives, false positives, and false negatives\n",
        "    tp = sum((a == 1 and b == 1) for (a, b) in zip(y_test, y_pred))\n",
        "    fp = sum((a == 0 and b == 1) for (a, b) in zip(y_test, y_pred))\n",
        "    fn = sum((a == 1 and b == 0) for (a, b) in zip(y_test, y_pred))\n",
        "\n",
        "    # Calculate precision and recall\n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / (tp + fn)\n",
        "\n",
        "    # Calculate F1 score\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "    return f1_score\n"
      ],
      "metadata": {
        "id": "6AwNgBjSHwRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-d-WLZQ60Sm",
        "outputId": "b361f4b8-cb9f-4a92-8b3b-8c0aafb6bf96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14780, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Implement logistic regression"
      ],
      "metadata": {
        "id": "3oIjwTgA_xZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression:\n",
        "    def __init__(self, learning_rate=0.001, n_iterations=30):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for i in range(self.n_iterations):\n",
        "            # Calculate the model\n",
        "            linear_model = np.dot(X, self.weights) + self.bias\n",
        "            y_pred = self._sigmoid(linear_model)\n",
        "\n",
        "            dw = (1/n_samples) * np.dot(X.T, (y_pred - y))\n",
        "            db = (1/n_samples) * np.sum(y_pred - y)\n",
        "\n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_model = np.dot(X, self.weights) + self.bias\n",
        "        y_pred = self._sigmoid(linear_model)\n",
        "        y_pred_cls = [1 if i > 0.5 else 0 for i in y_pred]\n",
        "        return np.array(y_pred_cls)\n",
        "\n",
        "    def _sigmoid(self, x):\n",
        "        return 1/(1 + np.exp(-x))"
      ],
      "metadata": {
        "id": "QhCksoWB_wYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train, y_train)\n",
        "y_train_pred_lr = log_reg.predict(X_train)\n",
        "y_val_pred_lr = log_reg.predict(X_val)\n",
        "\n",
        "print(f'Training score:   {str(f1_score(y_train_pred_lr, y_train))}%')\n",
        "print(f'Validation score: {str(f1_score(y_val_pred_lr, y_val))}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkMz5z-s52NE",
        "outputId": "1a08a49c-937e-4651-8667-4232a000d2f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training score:   0.9960075646144149%\n",
            "Validation score: 0.9970995810505962%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Use L1 regularization with gradient descent optimizer."
      ],
      "metadata": {
        "id": "Gfz4PYSuALic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegressionL1:\n",
        "    def __init__(self, learning_rate=0.1, n_iterations=10, l1_lambda = 0.1):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.l1_lambda = l1_lambda\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "        for i in range(self.n_iterations):\n",
        "            # Calculate the model\n",
        "            linear_model = np.dot(X, self.weights) + self.bias\n",
        "            y_pred = self._sigmoid(linear_model)\n",
        "\n",
        "            dw = (2 * np.dot(X.T, (y_pred - y))) + self.l1_lambda * np.sign(self.weights)\n",
        "            db = 2 * np.sum(y_pred - y) + self.l1_lambda * np.sign(self.bias)\n",
        "\n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "\n",
        "            return self.cost_l1(X, y, y_pred)\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_model = np.dot(X, self.weights) + self.bias\n",
        "        y_pred = self._sigmoid(linear_model)\n",
        "        y_pred_cls = [1 if i > 0.5 else 0 for i in y_pred]\n",
        "        return np.array(y_pred_cls)\n",
        "\n",
        "    def _sigmoid(self, x):\n",
        "        return 1/(1 + np.exp(-x))\n",
        "\n",
        "    def cost_l1(self, X, y, y_pred):\n",
        "\n",
        "      cost = np.mean((y - y_pred)**2) + self.l1_lambda * np.sum(np.abs(self.weights))\n",
        "      return cost\n"
      ],
      "metadata": {
        "id": "hI5usdHhAOhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lambdas = [0.1, 1]\n",
        "\n",
        "for i in lambdas:\n",
        "  log_reg_l1 = LogisticRegressionL1(l1_lambda = i)\n",
        "  cost = log_reg_l1.fit(X_train, y_train)\n",
        "  y_train_pred_l1 = log_reg_l1.predict(X_train)\n",
        "  y_val_pred_l1 = log_reg_l1.predict(X_val)\n",
        "\n",
        "  print(f' -- For lambda {i}')\n",
        "  print(f'Training score:   {str(f1_score(y_train_pred_l1, y_train))}%')\n",
        "  print(f'Validation score: {str(f1_score(y_val_pred_l1, y_val))}%')\n",
        "  print(f'Cost:', cost)\n",
        "  print(f'-----------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDYPwK2YPg2p",
        "outputId": "946862e0-1fd2-4397-cac2-2fd292a690e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-128-a11aed8916ed>:31: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1 + np.exp(-x))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " -- For lambda 0.1\n",
            "Training score:   0.9957877000842459%\n",
            "Validation score: 0.9974176888315042%\n",
            "Cost: 18595.52482217892\n",
            "-----------------------------\n",
            " -- For lambda 1\n",
            "Training score:   0.9957877000842459%\n",
            "Validation score: 0.9974176888315042%\n",
            "Cost: 185952.9982217892\n",
            "-----------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Use mini-batch gradient descent optimizer."
      ],
      "metadata": {
        "id": "IlYDHsIj5ncq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegressionMiniBatch:\n",
        "    def __init__(self, learning_rate=0.001, n_iterations=10, batch_size=10):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def mini_batch_fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "        num_batches = n_samples // self.batch_size\n",
        "\n",
        "        for i in range(self.n_iterations):\n",
        "            for j in range(num_batches):\n",
        "                # Select a mini-batch\n",
        "                start_idx = j * self.batch_size\n",
        "                end_idx = start_idx + self.batch_size\n",
        "                X_batch = X[start_idx:end_idx]\n",
        "                y_batch = y[start_idx:end_idx]\n",
        "\n",
        "                # Calculate the model\n",
        "                linear_model = np.dot(X_batch, self.weights) + self.bias\n",
        "                y_pred = self._sigmoid(linear_model)\n",
        "\n",
        "                dw = (1/self.batch_size) * np.dot(X_batch.T, (y_pred - y_batch))\n",
        "                db = (1/self.batch_size) * np.sum(y_pred - y_batch)\n",
        "\n",
        "                self.weights -= self.learning_rate * dw\n",
        "                self.bias -= self.learning_rate * db\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_model = np.dot(X, self.weights) + self.bias\n",
        "        y_pred = self._sigmoid(linear_model)\n",
        "        y_pred_cls = [1 if i > 0.5 else 0 for i in y_pred]\n",
        "        return np.array(y_pred_cls)\n",
        "\n",
        "    def _sigmoid(self, x):\n",
        "        return 1/(1 + np.exp(-x))\n"
      ],
      "metadata": {
        "id": "goMtjmxj9tdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_sizes = [5, 50, 500, 5000]\n",
        "for b in batch_sizes:\n",
        "  mini_batch = LogisticRegressionMiniBatch(batch_size=b)\n",
        "  mini_batch.mini_batch_fit(X_train, y_train)\n",
        "  y_train_pred_mb = mini_batch.predict(X_train)\n",
        "  y_val_pred_mb = mini_batch.predict(X_val)\n",
        "  print(f' -- For batch size {b}')\n",
        "  print(f' Training score:   {str(f1_score(y_train_pred_mb, y_train))}%')\n",
        "  print(f' Validation score: {str(f1_score(y_val_pred_mb, y_val))}%')\n",
        "  print(f'-----------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hoUGmpP96SE",
        "outputId": "d89ec085-0ea1-4603-c504-16ae4103b3af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " -- For batch size 5\n",
            " Training score:   0.9989478114478114%\n",
            " Validation score: 0.9993535875888817%\n",
            "-----------------------------\n",
            " -- For batch size 50\n",
            " Training score:   0.9974763406940063%\n",
            " Validation score: 0.9987080103359173%\n",
            "-----------------------------\n",
            " -- For batch size 500\n",
            " Training score:   0.9961130370837273%\n",
            " Validation score: 0.9970995810505962%\n",
            "-----------------------------\n",
            " -- For batch size 5000\n",
            " Training score:   0.9960042060988433%\n",
            " Validation score: 0.9970977104159949%\n",
            "-----------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclustion**\n",
        "- With smaller batch size, the number of updates of the model parameters increases during each iteration, so the model converges to the optimal parameters faster during each iteration."
      ],
      "metadata": {
        "id": "yZ-FWnpP4y5r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. RMS Prob optimizer and Adam optimizer"
      ],
      "metadata": {
        "id": "vvaU8_kp8SKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegressionRMS:\n",
        "    def __init__(self, learning_rate=0.001, n_iterations=30, beta=0.9, epsilon=1e-8):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.beta = beta\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "        self.cache_w = np.zeros(n_features)\n",
        "        self.cache_b = 0\n",
        "\n",
        "        for i in range(self.n_iterations):\n",
        "            # Calculate the model\n",
        "            linear_model = np.dot(X, self.weights) + self.bias\n",
        "            y_pred = self._sigmoid(linear_model)\n",
        "\n",
        "            dw = (1/n_samples) * np.dot(X.T, (y_pred - y))\n",
        "            db = (1/n_samples) * np.sum(y_pred - y)\n",
        "\n",
        "            self.cache_w = self.beta * self.cache_w + (1 - self.beta) * np.power(dw, 2)\n",
        "            self.cache_b = self.beta * self.cache_b + (1 - self.beta) * np.power(db, 2)\n",
        "\n",
        "            self.weights -= self.learning_rate * dw / (np.sqrt(self.cache_w) + self.epsilon)\n",
        "            self.bias -= self.learning_rate * db / (np.sqrt(self.cache_b) + self.epsilon)\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_model = np.dot(X, self.weights) + self.bias\n",
        "        y_pred = self._sigmoid(linear_model)\n",
        "        y_pred_cls = [1 if i > 0.5 else 0 for i in y_pred]\n",
        "        return np.array(y_pred_cls)\n",
        "\n",
        "    def _sigmoid(self, x):\n",
        "        return 1/(1 + np.exp(-x))\n"
      ],
      "metadata": {
        "id": "b3LbIG9P8XWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rms = LogisticRegressionRMS()\n",
        "rms.fit(X_train, y_train)\n",
        "y_train_pred_rms = rms.predict(X_train)\n",
        "y_val_pred_rms = rms.predict(X_val)\n",
        "print(f'-----------------------------')\n",
        "print(f' Training score:   {str(f1_score(y_train_pred_rms, y_train))}%')\n",
        "print(f' Validation score: {str(f1_score(y_val_pred_rms, y_val))}%')\n",
        "print(f'-----------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOyDH-sIcj3U",
        "outputId": "2fdce212-812a-43d0-acc5-21ff5c973081"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------\n",
            " Training score:   0.9956928248765627%\n",
            " Validation score: 0.9970995810505962%\n",
            "-----------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegressionADAM:\n",
        "    def __init__(self, learning_rate=0.001, n_iterations=30, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "        self.m_w = np.zeros(n_features)\n",
        "        self.v_w = np.zeros(n_features)\n",
        "        self.m_b = 0\n",
        "        self.v_b = 0\n",
        "        t = 0\n",
        "\n",
        "        for i in range(self.n_iterations):\n",
        "            # Calculate the model\n",
        "            linear_model = np.dot(X, self.weights) + self.bias\n",
        "            y_pred = self._sigmoid(linear_model)\n",
        "\n",
        "            dw = (1/n_samples) * np.dot(X.T, (y_pred - y))\n",
        "            db = (1/n_samples) * np.sum(y_pred - y)\n",
        "\n",
        "            t += 1\n",
        "            self.m_w = self.beta1 * self.m_w + (1 - self.beta1) * dw\n",
        "            self.v_w = self.beta2 * self.v_w + (1 - self.beta2) * np.power(dw, 2)\n",
        "            self.m_b = self.beta1 * self.m_b + (1 - self.beta1) * db\n",
        "            self.v_b = self.beta2 * self.v_b + (1 - self.beta2) * np.power(db, 2)\n",
        "\n",
        "            m_w_hat = self.m_w / (1 - np.power(self.beta1, t))\n",
        "            v_w_hat = self.v_w / (1 - np.power(self.beta2, t))\n",
        "            m_b_hat = self.m_b / (1 - np.power(self.beta1, t))\n",
        "            v_b_hat = self.v_b / (1 - np.power(self.beta2, t))\n",
        "\n",
        "            self.weights -= self.learning_rate * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
        "            self.bias -= self.learning_rate * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_model = np.dot(X, self.weights) + self.bias\n",
        "        y_pred = self._sigmoid(linear_model)\n",
        "        y_pred_cls = [1 if i > 0.5 else 0 for i in y_pred]\n",
        "        return np.array(y_pred_cls)\n",
        "\n",
        "    def _sigmoid(self, x):\n",
        "        return 1/(1 + np.exp(-x))\n"
      ],
      "metadata": {
        "id": "9zvRg7asdFqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adam = LogisticRegressionADAM()\n",
        "adam.fit(X_train, y_train)\n",
        "y_train_pred_adam = adam.predict(X_train)\n",
        "y_val_pred_adam = adam.predict(X_val)\n",
        "print(f'-----------------------------')\n",
        "print(f' Training score:   {str(f1_score(y_train_pred_adam, y_train))}%')\n",
        "print(f' Validation score: {str(f1_score(y_val_pred_adam, y_val))}%')\n",
        "print(f'-----------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDXT1arBdGpA",
        "outputId": "61997e63-b17a-459a-d0b4-efdfb03e1996"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------\n",
            " Training score:   0.9947512072223389%\n",
            " Validation score: 0.9964573268921095%\n",
            "-----------------------------\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Q39F7QqT8a_j",
        "zmpKp0BN86GA",
        "4j-5fPPu9tgp",
        "iU5aaqAy9wzY"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}