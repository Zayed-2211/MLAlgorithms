{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries"
      ],
      "metadata": {
        "id": "zdhiBhkOatll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# For preprocessing\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Word2Vec\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics.pairwise import euclidean_distances"
      ],
      "metadata": {
        "id": "f7Si1AITNOSo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d58176ad-c2c7-4c0e-b520-7cdf6a058e65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing function"
      ],
      "metadata": {
        "id": "vpsoyyhybFu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(sentence):\n",
        "# Define a translation table that maps punctuation characters to None\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "  # Define a set of stopwords\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "\n",
        "  # Remove punctuation\n",
        "  no_punct = sentence.translate(table)\n",
        "\n",
        "  # Remove non-alphabetic characters\n",
        "  words = re.sub(r'\\W', ' ', no_punct)\n",
        "\n",
        "  # Remove numbers\n",
        "  words = re.sub(r'\\d', ' ', words)\n",
        "\n",
        "  # Convert to lowercase\n",
        "  lower_case = words.lower()\n",
        "\n",
        "  # Tokenize the sentence\n",
        "  tokens = word_tokenize(lower_case)\n",
        "\n",
        "  # Remove stopwords\n",
        "  no_stops = [word for word in tokens if not word in stop_words]\n",
        "\n",
        "  # Join the words back into a single string\n",
        "  cleaned_sentence = ' '.join(no_stops)\n",
        "\n",
        "  return cleaned_sentence\n"
      ],
      "metadata": {
        "id": "MRzGLeWje9oV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding function"
      ],
      "metadata": {
        "id": "obS7DrHzbJiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate average word vector\n",
        "def average_word_vector(tokens, model, vocabulary, num_features):\n",
        "    feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
        "    ntokens = float(len(vocabulary))\n",
        "\n",
        "    for token in tokens:\n",
        "        if token in vocabulary:\n",
        "            feature_vector = feature_vector + model.wv[token]\n",
        "\n",
        "    if ntokens:\n",
        "        feature_vector = feature_vector / ntokens\n",
        "\n",
        "    return feature_vector"
      ],
      "metadata": {
        "id": "wNCZ5gHdNPTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_embedding(docname1, docname2):\n",
        "\n",
        "  # Read files\n",
        "  try:\n",
        "      doc1 = pd.read_csv(docname1, encoding='utf-8')\n",
        "  except UnicodeDecodeError:\n",
        "      doc1 = pd.read_csv(docname1, encoding='latin-1')\n",
        "  try:\n",
        "      doc2 = pd.read_csv(docname2, encoding='utf-8')\n",
        "  except UnicodeDecodeError:\n",
        "      doc2 = pd.read_csv(docname2, encoding='latin-1')\n",
        "\n",
        "  # Clean documents\n",
        "  clean_doc1 = clean_text(doc1.to_string())\n",
        "  clean_doc2 = clean_text(doc2.to_string())\n",
        "\n",
        "  # Tokenize documents\n",
        "  tokens1 = word_tokenize(clean_doc1)\n",
        "  tokens2 = word_tokenize(clean_doc2)\n",
        "\n",
        "  # Create a Word2Vec model\n",
        "  model = Word2Vec([tokens1, tokens2], min_count=1)\n",
        "\n",
        "  # Get the vocabulary\n",
        "  vocabulary = model.wv.index_to_key\n",
        "\n",
        "  # Get the word embeddings\n",
        "  embedding1 = average_word_vector(tokens1, model, vocabulary, num_features=100)\n",
        "  embedding2 = average_word_vector(tokens2, model, vocabulary, num_features=100)\n",
        "\n",
        "  return model, vocabulary, embedding1, embedding2, tokens1, tokens2"
      ],
      "metadata": {
        "id": "JuUMzi5Uk2Wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Similarity metrics function"
      ],
      "metadata": {
        "id": "IJIZTqlLbUzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate similarity metrics\n",
        "def calculate_similarity(vector1, vector2, tokens1, tokens2):\n",
        "\n",
        "    # Convert the vectors to sets\n",
        "    s1 = set(tokens1)\n",
        "    s2 = set(tokens2)\n",
        "\n",
        "    # Find the intersection and the union of the two sets\n",
        "    _intersection = s1.intersection(s2)\n",
        "    _union = s1.union(s2)\n",
        "\n",
        "    # Calculate the Jaccard similarity\n",
        "    jaccard_sim = len(_intersection) / len(_union)\n",
        "\n",
        "    cosine_sim = cosine_similarity(vector1.reshape(1, -1), vector2.reshape(1, -1))[0][0]\n",
        "    euclidean_dist = euclidean_distances(vector1.reshape(1, -1), vector2.reshape(1, -1))[0][0]\n",
        "\n",
        "    # Print similarity metrics\n",
        "    print(f'''----- Scores of similarity between documents: \"python\" and \"java\" -----\n",
        "    ------------------- Cosine similarity:  {cosine_sim} -------------------\n",
        "    ------------------- Jaccard similarity:  {jaccard_sim} -------------------\n",
        "    ------------------- Euclidean Distance: {euclidean_dist} ------------------''')"
      ],
      "metadata": {
        "id": "aayjKu7cSMgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1"
      ],
      "metadata": {
        "id": "6_wg8LpsbY5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the documents\n",
        "python = 'python.csv'\n",
        "python2 = 'python2.csv'\n",
        "java = 'java.csv'"
      ],
      "metadata": {
        "id": "vW2Dn7Rreqkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call function for task1\n",
        "model, vocabulary, embedding1, embedding2, tokens1, tokens2 = word_embedding(python, python2)\n",
        "\n",
        "for word in vocabulary[0:5]:\n",
        "    print(f\"{word}: {model.wv[word]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRtnJ1YBeZEV",
        "outputId": "8eb9fe14-eadc-4e23-a4c3-420080174a08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python: [-0.00077108  0.00051207  0.00520512  0.00910442 -0.0091779  -0.00749649\n",
            "  0.00668652  0.00967212 -0.00529421 -0.00399131  0.007214   -0.00192312\n",
            " -0.00473666  0.00639873 -0.00488523 -0.00205207  0.00294557  0.00065835\n",
            " -0.00829692 -0.00992456  0.00719405  0.00520265  0.00701148  0.00048711\n",
            "  0.00624098 -0.00329236 -0.00126264  0.00548296 -0.0077379  -0.0038523\n",
            " -0.00735089 -0.00081551  0.00984462 -0.00743861 -0.00271434 -0.0015621\n",
            "  0.00806709 -0.00624697 -0.00017764 -0.00533942 -0.00949866  0.00449026\n",
            " -0.00884654 -0.00433464  0.0001733  -0.00038111 -0.00779567  0.00937303\n",
            "  0.005176    0.00926685 -0.0077611   0.0042539  -0.00399676  0.00092271\n",
            "  0.00821121 -0.00413223  0.00452585 -0.00687609 -0.00407459  0.009441\n",
            " -0.00144991  0.00047071 -0.00401306 -0.00788953 -0.00190438  0.002545\n",
            " -0.00065995  0.0059661  -0.00323246  0.00256702  0.00517464  0.00849781\n",
            " -0.00118983 -0.00935283  0.00470852  0.00087295  0.00745849 -0.00084218\n",
            " -0.00285913 -0.00863171 -0.0009456   0.00290662  0.00494699  0.00737365\n",
            " -0.00595279  0.00189044  0.00618798 -0.00432748 -0.00272791  0.00657993\n",
            "  0.00219767  0.00042212  0.00336902  0.00021708  0.01015966  0.00551222\n",
            " -0.0088267  -0.00712295  0.00095482  0.00613476]\n",
            "language: [-8.7637259e-03  3.8519064e-03  5.2533257e-03  5.8314092e-03\n",
            "  7.5743767e-03 -6.4582815e-03  1.2704072e-03  6.5277456e-03\n",
            " -3.0321716e-03 -6.3438197e-03 -5.1947258e-04 -8.5997228e-03\n",
            " -5.7436260e-03  6.9867396e-03  3.3000051e-03  7.0713358e-03\n",
            "  6.8624751e-03  7.2819539e-03 -3.8332499e-03 -8.6938002e-04\n",
            "  2.2847578e-03 -4.4094818e-03  8.5819270e-03 -1.0042614e-02\n",
            "  6.6993488e-03  3.0031423e-03 -5.1803766e-03  4.1575232e-03\n",
            " -1.8960864e-03  6.7591830e-03  1.0051071e-02 -4.3034921e-03\n",
            " -3.9013525e-04 -5.7775881e-03  3.6284265e-03  3.0240614e-03\n",
            "  6.8861521e-03  5.9358529e-03  9.4000800e-03  8.8932430e-03\n",
            "  7.9810005e-03 -7.3439800e-03 -9.2351194e-03 -3.1609635e-04\n",
            " -2.9941478e-03  7.7983257e-03  5.8730133e-03 -1.6941994e-03\n",
            "  1.6545265e-03  1.8402190e-03  8.0575561e-03 -9.6643642e-03\n",
            " -9.6676718e-05  3.5157278e-03 -1.1294535e-03  8.5781775e-03\n",
            "  9.0542352e-03  6.4502931e-03 -1.0047150e-03  7.7466550e-03\n",
            " -8.4819729e-03  3.2963105e-03 -4.5335889e-03 -5.2487743e-03\n",
            "  3.3197431e-03  5.4318081e-03  7.9004094e-03 -5.4861931e-03\n",
            "  7.0912926e-03  6.8080695e-03 -3.8740935e-03 -8.6575551e-03\n",
            "  5.6309626e-03  6.4138444e-03 -5.5741030e-04 -6.5058623e-03\n",
            " -7.1062618e-03 -2.5384093e-03  4.9657347e-03 -3.5508443e-03\n",
            " -9.4273007e-03  3.9200652e-03  4.5991777e-03 -6.2583704e-03\n",
            "  1.0127672e-03 -2.0346348e-03  5.9517199e-05 -9.5736906e-03\n",
            "  2.9339150e-03 -4.8535801e-03  1.4796960e-03 -1.4122265e-03\n",
            "  2.1581259e-03 -7.8770788e-03 -2.3566901e-03  2.9646731e-03\n",
            "  5.4041627e-03 -2.3919318e-03 -9.4858371e-03  4.3063210e-03]\n",
            "used: [-1.4413957e-04  3.3764616e-03 -6.7257429e-03 -1.1849953e-03\n",
            "  7.7528902e-03  6.9516087e-03 -3.4480386e-03  3.3572910e-03\n",
            " -8.5909702e-03  6.0526934e-03 -4.8537245e-03 -3.5410421e-03\n",
            "  9.1150198e-03  7.3585490e-04  7.4465051e-03 -6.3534537e-03\n",
            "  5.2587790e-03  9.5737875e-03 -8.4861908e-03 -5.5733956e-03\n",
            " -7.1662776e-03 -4.7052326e-03 -3.4687999e-03 -8.8302875e-03\n",
            "  7.8782849e-03 -4.7825482e-03  8.0123236e-03  5.0007184e-03\n",
            " -6.7710695e-03  4.0635318e-03  5.5594048e-03 -7.3784161e-03\n",
            " -7.0580016e-03 -2.5790986e-03 -8.9678066e-03 -1.2280850e-03\n",
            " -3.8753514e-04  3.0032599e-03  1.2250523e-03 -1.4221952e-03\n",
            " -5.5184648e-03  1.2588134e-03 -9.4579760e-04  6.7766490e-03\n",
            "  4.1118306e-03  4.3798671e-03  1.2866720e-03 -2.9073863e-03\n",
            " -4.2018555e-03 -1.0204066e-03  1.7865057e-03 -2.9232081e-03\n",
            " -6.9460105e-03 -7.6914509e-03 -9.4013372e-03 -5.5906489e-03\n",
            " -1.8012925e-03 -4.4525317e-03 -6.9061839e-03 -3.6748033e-03\n",
            "  4.3731621e-03 -3.6192229e-03  8.5154483e-03  1.2900326e-03\n",
            " -7.6455716e-03  9.4856592e-03  7.9497034e-03  5.8735344e-03\n",
            " -7.3595881e-03  6.0912780e-03  3.7434499e-03  5.3253574e-03\n",
            "  4.5102285e-03  1.7766315e-03 -2.8491016e-03  8.6025111e-03\n",
            "  9.5838727e-03  3.7662734e-03 -3.0648387e-03  1.5084460e-04\n",
            "  1.1055650e-03 -8.3522927e-03 -8.6079156e-03  8.0289632e-05\n",
            "  9.8636316e-04 -5.7172203e-03 -4.7053467e-03 -6.9208858e-03\n",
            "  8.7328991e-03 -8.3872044e-05 -3.9343191e-03  5.9143957e-03\n",
            "  9.0767303e-03 -4.1555762e-03  8.4734811e-03  5.7816720e-03\n",
            "  5.9618107e-03  4.7168203e-04  8.2770996e-03 -7.3057641e-03]\n",
            "programming: [-0.00838763  0.00951756 -0.00020449 -0.00184444  0.00474478 -0.00448361\n",
            "  0.00296494  0.00740612  0.00589606 -0.00768866  0.00928447  0.00442612\n",
            "  0.00379031 -0.00641758  0.00844352 -0.00233507  0.0088469  -0.00569278\n",
            " -0.0081455   0.0064703   0.00154985 -0.00211018  0.00965804  0.00926402\n",
            " -0.00988923  0.00252974  0.00592039  0.00365835  0.00187225  0.00048482\n",
            "  0.00081385 -0.00378596 -0.00693408 -0.00215916  0.00365958  0.00912715\n",
            "  0.00925568 -0.00619874 -0.00952535  0.00934075  0.003532    0.00478216\n",
            "  0.00613477 -0.00281122  0.00754332  0.00276486  0.00277639 -0.00250309\n",
            " -0.00297026 -0.00236852  0.00460032 -0.00013261 -0.00947247 -0.00961781\n",
            " -0.00636647  0.00017124  0.00205729  0.00937737  0.00523225 -0.00425711\n",
            "  0.00033762  0.00510815  0.00782557 -0.00132748  0.0040322  -0.0057528\n",
            " -0.00058678  0.00841351 -0.00265153 -0.00943504  0.00559216 -0.00385073\n",
            " -0.00100673  0.00992825 -0.00201266 -0.00451667 -0.00537311  0.00692991\n",
            " -0.00592415  0.00215624 -0.00531116  0.0061981   0.00399933  0.00284158\n",
            " -0.00161599 -0.00271087  0.00910408  0.00554782 -0.00187084 -0.00960912\n",
            " -0.0070729  -0.00084735 -0.00087961 -0.00259674  0.01009445 -0.00015189\n",
            "  0.00598834 -0.0074998  -0.00245193 -0.00580663]\n",
            "code: [-7.3311864e-03  1.4427763e-03 -7.0869788e-03 -2.1521754e-03\n",
            "  3.8300951e-03  5.4919929e-03  1.3952781e-03  2.6662203e-03\n",
            " -4.3710014e-03  7.0642317e-03 -6.4904536e-03  4.3614092e-03\n",
            " -8.4436210e-03  1.8659785e-03 -4.9685556e-03 -4.4935090e-03\n",
            " -3.0573376e-03  5.3668953e-03  5.7979967e-03 -5.3728344e-03\n",
            "  6.9116452e-04 -8.4229708e-03  8.0643669e-03  8.9838076e-03\n",
            " -2.8152149e-03  8.4672117e-04  4.9173884e-04  5.1646382e-03\n",
            " -8.7843174e-03  6.8996556e-04  7.0405700e-03  2.3079053e-03\n",
            "  1.4030018e-03 -9.4118388e-03  8.1782788e-03 -5.9790998e-03\n",
            " -3.0204596e-03  3.2351066e-03 -9.7981223e-04  9.0943929e-04\n",
            "  1.8537762e-03 -7.2445241e-03 -9.7660059e-03  9.1092829e-03\n",
            "  6.3718520e-03 -6.9750343e-03  3.2596809e-03  5.1128951e-05\n",
            "  4.9714618e-03 -7.1015516e-03  4.3519186e-03  4.1918224e-03\n",
            "  1.0083918e-02 -4.3799500e-03 -1.6609149e-03 -7.0476555e-03\n",
            " -9.7300373e-03 -9.1585116e-03 -1.3852413e-03 -6.4837076e-03\n",
            "  4.9556619e-03 -6.0450956e-03  2.5747099e-03  5.2986178e-04\n",
            " -3.7790609e-03 -9.0246083e-04  1.0217746e-02  9.4623342e-03\n",
            " -4.9805343e-03  9.3208635e-03 -5.9191887e-03  6.0775271e-03\n",
            " -2.8719199e-03  3.3083248e-03  3.3028440e-03  7.1795802e-03\n",
            " -2.3902368e-03  8.7494645e-03  7.2939945e-03 -9.4492063e-03\n",
            " -8.0791004e-03 -7.5467438e-03  2.4991534e-03 -2.4762228e-03\n",
            " -7.2026667e-03 -8.0950847e-03  8.3408747e-03  2.3432162e-03\n",
            " -9.0523474e-03 -4.9571362e-03  3.5868802e-03 -4.5033395e-03\n",
            "  5.1289895e-03 -4.2522871e-03  3.0558889e-03 -7.6462254e-03\n",
            "  6.2852623e-03  4.7301394e-03  7.8383007e-04  2.7708693e-03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2"
      ],
      "metadata": {
        "id": "7fKjDToEbe-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task2\n",
        "\n",
        "# Calculate similarity metrics\n",
        "calculate_similarity(embedding1, embedding2, tokens1, tokens2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juaJn0SvdI_u",
        "outputId": "1c8c999e-6c17-48fa-9d68-c822610b38e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- Scores of similarity between documents: \"python\" and \"java\" -----\n",
            "    ------------------- Cosine similarity:  0.8409571288941643 -------------------\n",
            "    ------------------- Jaccard similarity:  0.2765957446808511 -------------------\n",
            "    ------------------- Euclidean Distance: 0.004803549169020451 ------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- cosine similarity is high because it takes in consideration the importance of each word, while the jaccard similarity is lower because it only considers if words occured or not and igores the frequency, importance and context of words, so in this kind of application the cosine similarity is better to be used."
      ],
      "metadata": {
        "id": "Pf37Smp_Z8ao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, vocabulary, embedding1, embedding2, tokens1, tokens2 = word_embedding(python, java)\n",
        "\n",
        "# Calculate similarity metrics\n",
        "calculate_similarity(embedding1, embedding2, tokens1, tokens2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-hHq03XnIea",
        "outputId": "ae7c9f0c-81d1-456e-98bd-f94856b9eb48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- Scores of similarity between documents: \"python\" and \"java\" -----\n",
            "    ------------------- Cosine similarity:  0.34333152680708895 -------------------\n",
            "    ------------------- Jaccard similarity:  0.17407407407407408 -------------------\n",
            "    ------------------- Euclidean Distance: 0.009474620319046397 ------------------\n"
          ]
        }
      ]
    }
  ]
}